{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04.Regularization_Dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNoj/aO0XkcSdZdqOKv/L4U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soohyunme/TensorFlow_Tutorial/blob/main/Code/04_Regularization_Dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kbvMfen8HHU-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from threading import active_count\n",
        "\n",
        "from tensorflow.python.keras.layers.core import Activation\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.datasets import cifar10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Device setting"
      ],
      "metadata": {
        "id": "lVyhR0vbEpVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0],True)"
      ],
      "metadata": {
        "id": "IZFQhWJFEZUB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "HdWMPOUeEuUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train,y_train), (x_test,y_test) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "WPt3ivEaEdB0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize"
      ],
      "metadata": {
        "id": "48Xn-xHFE6vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "pv2SrixNE6LB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "1LeauvNMFHLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_model():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "      inputs = keras.Input(shape=(32, 32, 3))\n",
        "      x = layers.Conv2D(\n",
        "          32, 3, padding='same', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = keras.activations.relu(x)\n",
        "      x = layers.MaxPooling2D()(x)\n",
        "      \n",
        "      x = layers.Conv2D(\n",
        "          64, 3, padding='same', kernel_regularizer=regularizers.l2(0.01) # regularizer 적용\n",
        "          )(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = keras.activations.relu(x)\n",
        "      x = layers.MaxPooling2D()(x)\n",
        "      \n",
        "      x = layers.Conv2D(\n",
        "          128, 3, padding='same', kernel_regularizer=regularizers.l2(0.01) # regularizer 적용\n",
        "          )(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = keras.activations.relu(x)\n",
        "      \n",
        "      x = layers.Flatten()(x)\n",
        "      x = layers.Dense(\n",
        "          64, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01) # regularizer 적용\n",
        "          )(x)\n",
        "      x = layers.Dropout(0.5)(x) # Dropout 적용\n",
        "      outputs = layers.Dense(10)(x)\n",
        "      model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "      return model"
      ],
      "metadata": {
        "id": "mUkwMzqPZ5Kf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = my_model()"
      ],
      "metadata": {
        "id": "fGPOAMl6KoJi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa_s0asmMMS9",
        "outputId": "76650489-0519-43da-9a1d-0a20d6e6d598"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " tf.nn.relu (TFOpLambda)     (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " tf.nn.relu_1 (TFOpLambda)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " tf.nn.relu_2 (TFOpLambda)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                524352    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 619,146\n",
            "Trainable params: 618,698\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model compile"
      ],
      "metadata": {
        "id": "ma7S5FwtIlNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer = keras.optimizers.Adam(lr=3e-4),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEN7xxAkHEL5",
        "outputId": "135240d3-e049-4e17-b3d6-63986caa8288"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and evaluate"
      ],
      "metadata": {
        "id": "mHzEfDtrIpxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  history = model.fit(x_train, y_train, batch_size=64, epochs=150, verbose=2)\n",
        "  model.evaluate(x_test, y_test, batch_size=64, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOB5sYz-lGZ5",
        "outputId": "77e042f6-9fda-422f-feaa-720b86cd86bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "782/782 - 19s - loss: 3.0549 - accuracy: 0.3575 - 19s/epoch - 24ms/step\n",
            "Epoch 2/150\n",
            "782/782 - 9s - loss: 1.9529 - accuracy: 0.4673 - 9s/epoch - 11ms/step\n",
            "Epoch 3/150\n",
            "782/782 - 9s - loss: 1.6501 - accuracy: 0.5156 - 9s/epoch - 11ms/step\n",
            "Epoch 4/150\n",
            "782/782 - 9s - loss: 1.5239 - accuracy: 0.5406 - 9s/epoch - 11ms/step\n",
            "Epoch 5/150\n",
            "782/782 - 9s - loss: 1.4564 - accuracy: 0.5584 - 9s/epoch - 11ms/step\n",
            "Epoch 6/150\n",
            "782/782 - 9s - loss: 1.4120 - accuracy: 0.5722 - 9s/epoch - 11ms/step\n",
            "Epoch 7/150\n",
            "782/782 - 9s - loss: 1.3822 - accuracy: 0.5833 - 9s/epoch - 11ms/step\n",
            "Epoch 8/150\n",
            "782/782 - 9s - loss: 1.3563 - accuracy: 0.5907 - 9s/epoch - 11ms/step\n",
            "Epoch 9/150\n",
            "782/782 - 9s - loss: 1.3374 - accuracy: 0.5973 - 9s/epoch - 11ms/step\n",
            "Epoch 10/150\n",
            "782/782 - 9s - loss: 1.3111 - accuracy: 0.6083 - 9s/epoch - 11ms/step\n",
            "Epoch 11/150\n",
            "782/782 - 9s - loss: 1.2950 - accuracy: 0.6132 - 9s/epoch - 11ms/step\n",
            "Epoch 12/150\n",
            "782/782 - 9s - loss: 1.2820 - accuracy: 0.6188 - 9s/epoch - 11ms/step\n",
            "Epoch 13/150\n",
            "782/782 - 9s - loss: 1.2714 - accuracy: 0.6246 - 9s/epoch - 11ms/step\n",
            "Epoch 14/150\n",
            "782/782 - 9s - loss: 1.2522 - accuracy: 0.6290 - 9s/epoch - 11ms/step\n",
            "Epoch 15/150\n",
            "782/782 - 9s - loss: 1.2368 - accuracy: 0.6357 - 9s/epoch - 11ms/step\n",
            "Epoch 16/150\n",
            "782/782 - 9s - loss: 1.2358 - accuracy: 0.6379 - 9s/epoch - 11ms/step\n",
            "Epoch 17/150\n",
            "782/782 - 9s - loss: 1.2154 - accuracy: 0.6473 - 9s/epoch - 11ms/step\n",
            "Epoch 18/150\n",
            "782/782 - 9s - loss: 1.2100 - accuracy: 0.6478 - 9s/epoch - 11ms/step\n",
            "Epoch 19/150\n",
            "782/782 - 9s - loss: 1.1926 - accuracy: 0.6576 - 9s/epoch - 11ms/step\n",
            "Epoch 20/150\n",
            "782/782 - 9s - loss: 1.1865 - accuracy: 0.6600 - 9s/epoch - 11ms/step\n",
            "Epoch 21/150\n",
            "782/782 - 9s - loss: 1.1940 - accuracy: 0.6609 - 9s/epoch - 11ms/step\n",
            "Epoch 22/150\n",
            "782/782 - 9s - loss: 1.1756 - accuracy: 0.6644 - 9s/epoch - 11ms/step\n",
            "Epoch 23/150\n",
            "782/782 - 9s - loss: 1.1674 - accuracy: 0.6727 - 9s/epoch - 11ms/step\n",
            "Epoch 24/150\n",
            "782/782 - 9s - loss: 1.1655 - accuracy: 0.6701 - 9s/epoch - 11ms/step\n",
            "Epoch 25/150\n",
            "782/782 - 9s - loss: 1.1532 - accuracy: 0.6751 - 9s/epoch - 11ms/step\n",
            "Epoch 26/150\n",
            "782/782 - 9s - loss: 1.1482 - accuracy: 0.6807 - 9s/epoch - 11ms/step\n",
            "Epoch 27/150\n",
            "782/782 - 9s - loss: 1.1416 - accuracy: 0.6846 - 9s/epoch - 11ms/step\n",
            "Epoch 28/150\n",
            "782/782 - 9s - loss: 1.1356 - accuracy: 0.6872 - 9s/epoch - 11ms/step\n",
            "Epoch 29/150\n",
            "782/782 - 9s - loss: 1.1299 - accuracy: 0.6899 - 9s/epoch - 11ms/step\n",
            "Epoch 30/150\n",
            "782/782 - 9s - loss: 1.1279 - accuracy: 0.6882 - 9s/epoch - 11ms/step\n",
            "Epoch 31/150\n",
            "782/782 - 9s - loss: 1.1138 - accuracy: 0.6957 - 9s/epoch - 11ms/step\n",
            "Epoch 32/150\n",
            "782/782 - 9s - loss: 1.1117 - accuracy: 0.6986 - 9s/epoch - 11ms/step\n",
            "Epoch 33/150\n",
            "782/782 - 9s - loss: 1.0975 - accuracy: 0.7035 - 9s/epoch - 11ms/step\n",
            "Epoch 34/150\n",
            "782/782 - 9s - loss: 1.0986 - accuracy: 0.7016 - 9s/epoch - 11ms/step\n",
            "Epoch 35/150\n",
            "782/782 - 9s - loss: 1.0966 - accuracy: 0.7053 - 9s/epoch - 11ms/step\n",
            "Epoch 36/150\n",
            "782/782 - 9s - loss: 1.0881 - accuracy: 0.7102 - 9s/epoch - 11ms/step\n",
            "Epoch 37/150\n",
            "782/782 - 9s - loss: 1.0840 - accuracy: 0.7087 - 9s/epoch - 11ms/step\n",
            "Epoch 38/150\n",
            "782/782 - 9s - loss: 1.0768 - accuracy: 0.7124 - 9s/epoch - 11ms/step\n",
            "Epoch 39/150\n",
            "782/782 - 9s - loss: 1.0832 - accuracy: 0.7102 - 9s/epoch - 11ms/step\n",
            "Epoch 40/150\n",
            "782/782 - 9s - loss: 1.0741 - accuracy: 0.7166 - 9s/epoch - 11ms/step\n",
            "Epoch 41/150\n",
            "782/782 - 9s - loss: 1.0756 - accuracy: 0.7160 - 9s/epoch - 11ms/step\n",
            "Epoch 42/150\n",
            "782/782 - 9s - loss: 1.0612 - accuracy: 0.7213 - 9s/epoch - 11ms/step\n",
            "Epoch 43/150\n",
            "782/782 - 9s - loss: 1.0553 - accuracy: 0.7243 - 9s/epoch - 11ms/step\n",
            "Epoch 44/150\n",
            "782/782 - 9s - loss: 1.0631 - accuracy: 0.7185 - 9s/epoch - 11ms/step\n",
            "Epoch 45/150\n",
            "782/782 - 9s - loss: 1.0625 - accuracy: 0.7216 - 9s/epoch - 11ms/step\n",
            "Epoch 46/150\n",
            "782/782 - 9s - loss: 1.0483 - accuracy: 0.7282 - 9s/epoch - 11ms/step\n",
            "Epoch 47/150\n",
            "782/782 - 9s - loss: 1.0487 - accuracy: 0.7269 - 9s/epoch - 11ms/step\n",
            "Epoch 48/150\n",
            "782/782 - 9s - loss: 1.0415 - accuracy: 0.7302 - 9s/epoch - 11ms/step\n",
            "Epoch 49/150\n",
            "782/782 - 9s - loss: 1.0408 - accuracy: 0.7300 - 9s/epoch - 11ms/step\n",
            "Epoch 50/150\n",
            "782/782 - 9s - loss: 1.0345 - accuracy: 0.7349 - 9s/epoch - 11ms/step\n",
            "Epoch 51/150\n",
            "782/782 - 9s - loss: 1.0267 - accuracy: 0.7360 - 9s/epoch - 11ms/step\n",
            "Epoch 52/150\n",
            "782/782 - 9s - loss: 1.0279 - accuracy: 0.7367 - 9s/epoch - 11ms/step\n",
            "Epoch 53/150\n",
            "782/782 - 9s - loss: 1.0267 - accuracy: 0.7375 - 9s/epoch - 11ms/step\n",
            "Epoch 54/150\n",
            "782/782 - 9s - loss: 1.0302 - accuracy: 0.7348 - 9s/epoch - 11ms/step\n",
            "Epoch 55/150\n",
            "782/782 - 9s - loss: 1.0263 - accuracy: 0.7355 - 9s/epoch - 11ms/step\n",
            "Epoch 56/150\n",
            "782/782 - 9s - loss: 1.0238 - accuracy: 0.7388 - 9s/epoch - 11ms/step\n",
            "Epoch 57/150\n",
            "782/782 - 9s - loss: 1.0184 - accuracy: 0.7400 - 9s/epoch - 11ms/step\n",
            "Epoch 58/150\n",
            "782/782 - 9s - loss: 1.0048 - accuracy: 0.7467 - 9s/epoch - 11ms/step\n",
            "Epoch 59/150\n",
            "782/782 - 9s - loss: 1.0170 - accuracy: 0.7413 - 9s/epoch - 11ms/step\n",
            "Epoch 60/150\n",
            "782/782 - 9s - loss: 1.0101 - accuracy: 0.7449 - 9s/epoch - 11ms/step\n",
            "Epoch 61/150\n",
            "782/782 - 9s - loss: 1.0051 - accuracy: 0.7429 - 9s/epoch - 11ms/step\n",
            "Epoch 62/150\n",
            "782/782 - 9s - loss: 1.0050 - accuracy: 0.7453 - 9s/epoch - 11ms/step\n",
            "Epoch 63/150\n",
            "782/782 - 9s - loss: 1.0043 - accuracy: 0.7461 - 9s/epoch - 11ms/step\n",
            "Epoch 64/150\n",
            "782/782 - 9s - loss: 1.0020 - accuracy: 0.7465 - 9s/epoch - 12ms/step\n",
            "Epoch 65/150\n",
            "782/782 - 11s - loss: 1.0016 - accuracy: 0.7474 - 11s/epoch - 14ms/step\n",
            "Epoch 66/150\n",
            "782/782 - 10s - loss: 0.9953 - accuracy: 0.7491 - 10s/epoch - 13ms/step\n",
            "Epoch 67/150\n",
            "782/782 - 9s - loss: 0.9943 - accuracy: 0.7525 - 9s/epoch - 11ms/step\n",
            "Epoch 68/150\n",
            "782/782 - 9s - loss: 0.9874 - accuracy: 0.7535 - 9s/epoch - 11ms/step\n",
            "Epoch 69/150\n",
            "782/782 - 9s - loss: 0.9897 - accuracy: 0.7510 - 9s/epoch - 11ms/step\n",
            "Epoch 70/150\n",
            "782/782 - 9s - loss: 0.9948 - accuracy: 0.7502 - 9s/epoch - 11ms/step\n",
            "Epoch 71/150\n",
            "782/782 - 9s - loss: 0.9837 - accuracy: 0.7550 - 9s/epoch - 11ms/step\n",
            "Epoch 72/150\n",
            "782/782 - 9s - loss: 0.9854 - accuracy: 0.7536 - 9s/epoch - 11ms/step\n",
            "Epoch 73/150\n",
            "782/782 - 9s - loss: 0.9810 - accuracy: 0.7572 - 9s/epoch - 11ms/step\n",
            "Epoch 74/150\n",
            "782/782 - 9s - loss: 0.9834 - accuracy: 0.7546 - 9s/epoch - 11ms/step\n",
            "Epoch 75/150\n",
            "782/782 - 9s - loss: 0.9834 - accuracy: 0.7561 - 9s/epoch - 11ms/step\n",
            "Epoch 76/150\n",
            "782/782 - 9s - loss: 0.9764 - accuracy: 0.7586 - 9s/epoch - 12ms/step\n",
            "Epoch 77/150\n",
            "782/782 - 9s - loss: 0.9753 - accuracy: 0.7591 - 9s/epoch - 11ms/step\n",
            "Epoch 78/150\n",
            "782/782 - 9s - loss: 0.9807 - accuracy: 0.7579 - 9s/epoch - 11ms/step\n",
            "Epoch 79/150\n",
            "782/782 - 9s - loss: 0.9787 - accuracy: 0.7588 - 9s/epoch - 11ms/step\n",
            "Epoch 80/150\n",
            "782/782 - 9s - loss: 0.9740 - accuracy: 0.7585 - 9s/epoch - 11ms/step\n",
            "Epoch 81/150\n",
            "782/782 - 9s - loss: 0.9729 - accuracy: 0.7592 - 9s/epoch - 11ms/step\n",
            "Epoch 82/150\n",
            "782/782 - 9s - loss: 0.9753 - accuracy: 0.7589 - 9s/epoch - 11ms/step\n",
            "Epoch 83/150\n",
            "782/782 - 9s - loss: 0.9608 - accuracy: 0.7634 - 9s/epoch - 11ms/step\n",
            "Epoch 84/150\n",
            "782/782 - 9s - loss: 0.9678 - accuracy: 0.7605 - 9s/epoch - 11ms/step\n",
            "Epoch 85/150\n",
            "782/782 - 9s - loss: 0.9615 - accuracy: 0.7640 - 9s/epoch - 11ms/step\n",
            "Epoch 86/150\n",
            "782/782 - 9s - loss: 0.9685 - accuracy: 0.7584 - 9s/epoch - 11ms/step\n",
            "Epoch 87/150\n",
            "782/782 - 9s - loss: 0.9679 - accuracy: 0.7624 - 9s/epoch - 11ms/step\n",
            "Epoch 88/150\n",
            "782/782 - 9s - loss: 0.9623 - accuracy: 0.7642 - 9s/epoch - 11ms/step\n",
            "Epoch 89/150\n",
            "782/782 - 9s - loss: 0.9549 - accuracy: 0.7663 - 9s/epoch - 11ms/step\n",
            "Epoch 90/150\n",
            "782/782 - 9s - loss: 0.9571 - accuracy: 0.7652 - 9s/epoch - 11ms/step\n",
            "Epoch 91/150\n",
            "782/782 - 9s - loss: 0.9585 - accuracy: 0.7666 - 9s/epoch - 11ms/step\n",
            "Epoch 92/150\n",
            "782/782 - 9s - loss: 0.9516 - accuracy: 0.7692 - 9s/epoch - 11ms/step\n",
            "Epoch 93/150\n",
            "782/782 - 9s - loss: 0.9604 - accuracy: 0.7653 - 9s/epoch - 11ms/step\n",
            "Epoch 94/150\n",
            "782/782 - 9s - loss: 0.9561 - accuracy: 0.7696 - 9s/epoch - 11ms/step\n",
            "Epoch 95/150\n",
            "782/782 - 9s - loss: 0.9513 - accuracy: 0.7703 - 9s/epoch - 11ms/step\n",
            "Epoch 96/150\n",
            "782/782 - 9s - loss: 0.9511 - accuracy: 0.7687 - 9s/epoch - 11ms/step\n",
            "Epoch 97/150\n",
            "782/782 - 9s - loss: 0.9476 - accuracy: 0.7704 - 9s/epoch - 11ms/step\n",
            "Epoch 98/150\n",
            "782/782 - 9s - loss: 0.9537 - accuracy: 0.7676 - 9s/epoch - 11ms/step\n",
            "Epoch 99/150\n",
            "782/782 - 9s - loss: 0.9493 - accuracy: 0.7710 - 9s/epoch - 11ms/step\n",
            "Epoch 100/150\n",
            "782/782 - 9s - loss: 0.9571 - accuracy: 0.7680 - 9s/epoch - 11ms/step\n",
            "Epoch 101/150\n",
            "782/782 - 9s - loss: 0.9453 - accuracy: 0.7698 - 9s/epoch - 11ms/step\n",
            "Epoch 102/150\n",
            "782/782 - 9s - loss: 0.9484 - accuracy: 0.7702 - 9s/epoch - 11ms/step\n",
            "Epoch 103/150\n",
            "782/782 - 9s - loss: 0.9471 - accuracy: 0.7708 - 9s/epoch - 11ms/step\n",
            "Epoch 104/150\n",
            "782/782 - 9s - loss: 0.9438 - accuracy: 0.7720 - 9s/epoch - 11ms/step\n",
            "Epoch 105/150\n",
            "782/782 - 9s - loss: 0.9389 - accuracy: 0.7729 - 9s/epoch - 11ms/step\n",
            "Epoch 106/150\n",
            "782/782 - 9s - loss: 0.9421 - accuracy: 0.7731 - 9s/epoch - 11ms/step\n",
            "Epoch 107/150\n",
            "782/782 - 9s - loss: 0.9403 - accuracy: 0.7735 - 9s/epoch - 11ms/step\n",
            "Epoch 108/150\n",
            "782/782 - 9s - loss: 0.9397 - accuracy: 0.7751 - 9s/epoch - 11ms/step\n",
            "Epoch 109/150\n",
            "782/782 - 9s - loss: 0.9356 - accuracy: 0.7763 - 9s/epoch - 11ms/step\n",
            "Epoch 110/150\n",
            "782/782 - 9s - loss: 0.9428 - accuracy: 0.7735 - 9s/epoch - 11ms/step\n",
            "Epoch 111/150\n",
            "782/782 - 9s - loss: 0.9412 - accuracy: 0.7752 - 9s/epoch - 11ms/step\n",
            "Epoch 112/150\n",
            "782/782 - 9s - loss: 0.9413 - accuracy: 0.7749 - 9s/epoch - 11ms/step\n",
            "Epoch 113/150\n",
            "782/782 - 9s - loss: 0.9362 - accuracy: 0.7755 - 9s/epoch - 11ms/step\n",
            "Epoch 114/150\n",
            "782/782 - 9s - loss: 0.9313 - accuracy: 0.7776 - 9s/epoch - 11ms/step\n",
            "Epoch 115/150\n",
            "782/782 - 9s - loss: 0.9373 - accuracy: 0.7742 - 9s/epoch - 11ms/step\n",
            "Epoch 116/150\n",
            "782/782 - 9s - loss: 0.9397 - accuracy: 0.7755 - 9s/epoch - 11ms/step\n",
            "Epoch 117/150\n",
            "782/782 - 9s - loss: 0.9310 - accuracy: 0.7776 - 9s/epoch - 11ms/step\n",
            "Epoch 118/150\n",
            "782/782 - 9s - loss: 0.9324 - accuracy: 0.7787 - 9s/epoch - 11ms/step\n",
            "Epoch 119/150\n",
            "782/782 - 9s - loss: 0.9337 - accuracy: 0.7789 - 9s/epoch - 11ms/step\n",
            "Epoch 120/150\n",
            "782/782 - 9s - loss: 0.9272 - accuracy: 0.7786 - 9s/epoch - 11ms/step\n",
            "Epoch 121/150\n",
            "782/782 - 9s - loss: 0.9265 - accuracy: 0.7810 - 9s/epoch - 11ms/step\n",
            "Epoch 122/150\n",
            "782/782 - 9s - loss: 0.9322 - accuracy: 0.7801 - 9s/epoch - 11ms/step\n",
            "Epoch 123/150\n",
            "782/782 - 9s - loss: 0.9260 - accuracy: 0.7778 - 9s/epoch - 11ms/step\n",
            "Epoch 124/150\n",
            "782/782 - 9s - loss: 0.9229 - accuracy: 0.7833 - 9s/epoch - 11ms/step\n",
            "Epoch 125/150\n",
            "782/782 - 9s - loss: 0.9303 - accuracy: 0.7799 - 9s/epoch - 11ms/step\n",
            "Epoch 126/150\n",
            "782/782 - 9s - loss: 0.9328 - accuracy: 0.7773 - 9s/epoch - 11ms/step\n",
            "Epoch 127/150\n",
            "782/782 - 9s - loss: 0.9241 - accuracy: 0.7820 - 9s/epoch - 11ms/step\n",
            "Epoch 128/150\n",
            "782/782 - 9s - loss: 0.9249 - accuracy: 0.7805 - 9s/epoch - 11ms/step\n",
            "Epoch 129/150\n",
            "782/782 - 9s - loss: 0.9250 - accuracy: 0.7801 - 9s/epoch - 11ms/step\n",
            "Epoch 130/150\n",
            "782/782 - 9s - loss: 0.9329 - accuracy: 0.7772 - 9s/epoch - 11ms/step\n",
            "Epoch 131/150\n",
            "782/782 - 9s - loss: 0.9194 - accuracy: 0.7846 - 9s/epoch - 11ms/step\n",
            "Epoch 132/150\n",
            "782/782 - 9s - loss: 0.9235 - accuracy: 0.7814 - 9s/epoch - 11ms/step\n",
            "Epoch 133/150\n",
            "782/782 - 9s - loss: 0.9260 - accuracy: 0.7811 - 9s/epoch - 11ms/step\n",
            "Epoch 134/150\n",
            "782/782 - 9s - loss: 0.9273 - accuracy: 0.7791 - 9s/epoch - 11ms/step\n",
            "Epoch 135/150\n",
            "782/782 - 9s - loss: 0.9157 - accuracy: 0.7852 - 9s/epoch - 11ms/step\n",
            "Epoch 136/150\n",
            "782/782 - 9s - loss: 0.9255 - accuracy: 0.7834 - 9s/epoch - 11ms/step\n",
            "Epoch 137/150\n",
            "782/782 - 8s - loss: 0.9269 - accuracy: 0.7815 - 8s/epoch - 11ms/step\n",
            "Epoch 138/150\n",
            "782/782 - 8s - loss: 0.9226 - accuracy: 0.7859 - 8s/epoch - 11ms/step\n",
            "Epoch 139/150\n",
            "782/782 - 9s - loss: 0.9238 - accuracy: 0.7819 - 9s/epoch - 11ms/step\n",
            "Epoch 140/150\n",
            "782/782 - 8s - loss: 0.9132 - accuracy: 0.7845 - 8s/epoch - 11ms/step\n",
            "Epoch 141/150\n",
            "782/782 - 8s - loss: 0.9128 - accuracy: 0.7859 - 8s/epoch - 11ms/step\n",
            "Epoch 142/150\n",
            "782/782 - 8s - loss: 0.9223 - accuracy: 0.7837 - 8s/epoch - 11ms/step\n",
            "Epoch 143/150\n",
            "782/782 - 9s - loss: 0.9154 - accuracy: 0.7882 - 9s/epoch - 11ms/step\n",
            "Epoch 144/150\n",
            "782/782 - 9s - loss: 0.9205 - accuracy: 0.7863 - 9s/epoch - 11ms/step\n",
            "Epoch 145/150\n",
            "782/782 - 8s - loss: 0.9143 - accuracy: 0.7890 - 8s/epoch - 11ms/step\n",
            "Epoch 146/150\n",
            "782/782 - 9s - loss: 0.9197 - accuracy: 0.7877 - 9s/epoch - 11ms/step\n",
            "Epoch 147/150\n",
            "782/782 - 8s - loss: 0.9125 - accuracy: 0.7897 - 8s/epoch - 11ms/step\n",
            "Epoch 148/150\n",
            "782/782 - 8s - loss: 0.9184 - accuracy: 0.7857 - 8s/epoch - 11ms/step\n",
            "Epoch 149/150\n",
            "782/782 - 8s - loss: 0.9189 - accuracy: 0.7861 - 8s/epoch - 11ms/step\n",
            "Epoch 150/150\n",
            "782/782 - 8s - loss: 0.9176 - accuracy: 0.7883 - 8s/epoch - 11ms/step\n",
            "157/157 - 1s - loss: 1.1116 - accuracy: 0.7435 - 1s/epoch - 8ms/step\n"
          ]
        }
      ]
    }
  ]
}